{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=6 color=#0099ff> 0. Importing packages </font>\n",
    "\n",
    "<font face=\"Arial\" size=5 color=#DC143C> tips: </font>\n",
    "1. Organize packages in order (system and io packages, basic packages, statistics packages, plot packages, other packages.)\n",
    "2. ALWAYS add and revise on the same template and put it into Dash snippets. \n",
    "\n",
    "<font face=\"Arial\" size=5 color=#DC143C> why: </font>\n",
    "1. easy to manage packages in the enviroment (clear overview of what are in the running enviroment)\n",
    "2. make your code tidy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:33:38.420754Z",
     "start_time": "2020-01-15T12:33:38.107356Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# system and io packages\n",
    "import sys\n",
    "import cPickle as pickle\n",
    "\n",
    "#Basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#statistics packages\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import spearmanr\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "%load_ext rpy2.ipython\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "\n",
    "#plot packages\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot,iplot_mpl\n",
    "from plotly.graph_objs import *\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\n",
    "# other packages\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=6 color=#0099ff> 1.Import and combine Edata and basic demography data（age,gender...)\n",
    "</font> \n",
    "<font face=\"Arial\" size=5 color=#DC143C> tips: </font>\n",
    "1. load and save data by using read_csv() and to_csv() \n",
    "2. merge table by using df1.set_index(’Subject’).join(df2.set_index(’Subject’)) by index, also can use 'on=column name' \n",
    "3. setting 'Subject ID' as index is convenient \n",
    "4. after manipulating data (e.g. join, merge, groupby), ALWAYS check the resulting tables by using df.describe() and head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Import behavioral data(excel version of Edata)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:33:53.108381Z",
     "start_time": "2020-01-15T12:33:38.424980Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read excel version of Edata, pandas.read_excel\n",
    "se_all = pd.read_excel('SE_all.xlsx')\n",
    "raw_data = se_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:33:53.116194Z",
     "start_time": "2020-01-15T12:33:53.111521Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#atb_list = ['ExperimentName','Subject','Block','answer.ACC','answer.CRESP',  \n",
    "#            'Trigger.OnsetTime','answer.OnsetTime','answer.RESP','answer.RT',  \n",
    "#            'answer.RTTime','answer1','answer2','answer3','answer4','condition',\n",
    "#            'FA','repot.RESP','repot.RT','repot.RTTime','rightA',  \n",
    "#            'run','sen','sen_no','word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:33:53.575179Z",
     "start_time": "2020-01-15T12:33:53.120010Z"
    }
   },
   "outputs": [],
   "source": [
    "#set_index, rename, join\n",
    "data = raw_data.set_index(['Subject'])\n",
    "# replace wrong named data including {63:20,99:11,152:15,217:17}\n",
    "replaced_pairs = {63:20,99:11,152:15,217:17}\n",
    "data = data.rename(replaced_pairs, axis='index')\n",
    "se_basic_infor = pd.read_excel('SE_basic_infor.xlsx')\n",
    "se_basic_infor = se_basic_infor.set_index('Subject')\n",
    "data = data.join(se_basic_infor)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suject 24(absent),53(not follow the instruction),99,152,217 not clear what is the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=6 color=#0099ff>  2. Check response data (RT and response) for all the trials from each subject </font>\n",
    "\n",
    "<font face=\"Arial\" size=5 color=#DC143C> tips: </font>\n",
    "1. before average RTs cross trials, look into the distribution (pattern) of trial-trial RTs. detect outliers and address them properly (subject 52)\n",
    "2. use @interact from ipywidgets to check the histogram interactively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:34:06.067128Z",
     "start_time": "2020-01-15T12:34:05.951423Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#I think that we should not remove acc very low, maybe false alarm very high.\n",
    "#52, 53, 54, 56 for pre, \n",
    "# for unp 3, 11, 14, 15, 17, 18, 19, 20, 21, 23, 25, 26, 31, 32, 33, 36, 39,\n",
    "#            40, 41, 42, 44, 45, 47, 50, 52, 54, 56, 57, 58, 60, 61, 62\n",
    "# opt['acc_pre'] = bd_pre['answer.ACC'].mean(level='Subject')\n",
    "# opt[opt['acc_unp']<0.25].index \n",
    "# opt['fa_unp']/60\n",
    "#sub42(30 zeros)，44(60 zeros)，52(90 and more zeros)\n",
    "@interact(index_name=data.index.unique())\n",
    "def iplot_scale(index_name):\n",
    "    df = data[data.index==index_name]['answer.RT']\n",
    "    iplot(df.iplot(asFigure=True,kind='histogram', bins = 20,subplots=True, shape=(1, 1), filename='histogram-subplots'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font face=\"Arial\" size=6 color=#0099ff>  3. Average RT and ACC for each subject. </font>\n",
    "\n",
    "<font face=\"Arial\" size=5 color=#DC143C> tips: </font>\n",
    "1. df[df['column_name']] to select column\n",
    "2. df.mean(level='Sujbect') to average rt for each subject cross trials. df.fun() (sum, std, sqrt...)to manipulate the data. \n",
    "3. pd.concat((df1,df2),axis=1) to concat RT, ACC, False alarm and other variables into one table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:36:43.161341Z",
     "start_time": "2020-01-15T12:36:42.395545Z"
    }
   },
   "outputs": [],
   "source": [
    "#to check RT distribution for each subject and detect outliers\n",
    "opt =  pd.DataFrame()\n",
    "bd_pre =  data[data['condition']=='pre']\n",
    "bd_unp =  data[data['condition']=='unp']\n",
    "# rt is zero in some trials, exluding them.\n",
    "\n",
    "opt['acc_pre'] = bd_pre['answer.ACC'].mean(level='Subject')\n",
    "opt['acc_unp'] = bd_unp['answer.ACC'].mean(level='Subject')\n",
    "\n",
    "\n",
    "# only for all RIGHT response \n",
    "opt['conf_pre'] = bd_pre[bd_pre['answer.ACC']==1]['answer.RESP'].mean(level='Subject')\n",
    "opt['conf_unp'] = bd_unp[bd_unp['answer.ACC']==1]['answer.RESP'].mean(level='Subject')\n",
    "\n",
    "# only for all RIGHT response \n",
    "#opt['conf_pre'] = bd_pre['answer.RESP'].mean(level='Subject')\n",
    "#opt['conf_unp'] = bd_unp['answer.RESP'].mean(level='Subject')\n",
    "\n",
    "\n",
    "opt['rt_pre'] = bd_pre[bd_pre['answer.ACC']==1]['answer.RT'].mean(level='Subject') #\n",
    "opt['rt_unp'] = bd_unp[bd_unp['answer.ACC']==1]['answer.RT'].mean(level='Subject') #[bd_unp['answer.ACC']==1]\n",
    "#opt['rt_sd_pre'] = bd_pre[bd_pre['answer.ACC']==1]['answer.RT'].std(level='Subject')\n",
    "#opt['rt_sd_unp'] = bd_unp[bd_unp['answer.ACC']==1]['answer.RT'].std(level='Subject')\n",
    "\n",
    "# add false alarm rate\n",
    "bd_unp_ans_fa = bd_unp[bd_unp['answer.RESP']==bd_unp['FA']][['answer.RESP']]\n",
    "bd_unp_ans_fa['count_num'] = 1\n",
    "opt['fa_unp'] = bd_unp_ans_fa['count_num'].sum(level='Subject')\n",
    "opt['fa_pre'] = 0\n",
    "\n",
    "#concat different condtions and different measurements. \n",
    "\n",
    "acc = pd.concat((opt['acc_pre'],opt['acc_unp']),axis = 0)  \n",
    "conf = pd.concat((opt['conf_pre'],opt['conf_unp']),axis = 0)                  \n",
    "rt = pd.concat((opt['rt_pre'],opt['rt_unp']),axis = 0) \n",
    "fa = pd.concat((opt['fa_pre'],opt['fa_unp']),axis = 0) \n",
    "opt_all = pd.concat((acc,rt,fa,conf),axis=1)\n",
    "opt_all = opt_all.rename({0:'acc',1:'rt',2:'fa',3:'conf'},axis = 'columns')\n",
    "opt_all['condition'] = ['pre']*61+['unp']*61\n",
    "opt_all = opt_all.join(se_basic_infor)\n",
    "opt_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on scale score to devide all subject into two groups. add a group variable. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:36:43.865597Z",
     "start_time": "2020-01-15T12:36:43.840310Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = opt.join(se_basic_infor)\n",
    "#print(opt)\n",
    "LSHS_AVH_median = opt['LSHS_AVH'].median()\n",
    "group = []\n",
    "for i in opt.index:\n",
    "    if opt['LSHS_AVH'][i]<=LSHS_AVH_median:\n",
    "        group.append('Low')\n",
    "    else:\n",
    "        group.append('High')\n",
    "opt['group'] = group \n",
    "opt.dropna(how='any')\n",
    "#opt[['LSHS_AVH','group']]\n",
    "opt_all = opt_all.join(opt['group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T11:11:08.185508Z",
     "start_time": "2019-11-14T11:11:08.178403Z"
    }
   },
   "source": [
    "<font face=\"Arial\" size=5 color=#DC143C> tips: </font>\n",
    "1. df.iloc[i,j] to select the subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:36:45.124060Z",
     "start_time": "2020-01-15T12:36:44.966998Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine with questionaire data\n",
    "\n",
    "se_scale = pd.read_excel('SE_scale.xlsx')\n",
    "se_scale  = se_scale.set_index('Subject')\n",
    "#print(se_scale)\n",
    "#se_scale = se_scale.set_index(['Subject'])\n",
    "se_scale = se_scale.iloc[:,7:76]\n",
    "opt_all = opt_all.join(se_scale)\n",
    "se_qpe = pd.read_excel('SE-QPE_40.xlsx')\n",
    "se_qpe = se_qpe.set_index('Subject')\n",
    "QPE_severity = pd.DataFrame(se_qpe.iloc[:,12:16].sum(axis = 1),columns = ['QPE_severity'])\n",
    "opt_all = opt_all.join(QPE_severity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=6 color=#0099ff>  4. get sum scores of all iterms for each questionires. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:37:43.888240Z",
     "start_time": "2020-01-15T12:37:43.853064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate and summary the sales into sum scores.\n",
    "#print(opt_all.columns)\n",
    "stai_all = range(1,21)\n",
    "stai_all_opp = [1,3,6,7,10,13,14,16,19]\n",
    "stai_all_pos = list(set(stai_all).difference(set(stai_all_opp)))\n",
    "\n",
    "STAI_oppsiteiterm_colnames = ['STAI'+str(i) for i in stai_all_opp]\n",
    "STAI_positerm_colnames = ['STAI'+str(i) for i in stai_all_pos]\n",
    "opt_all['TAI_all'] = 45-opt_all.loc[:,STAI_oppsiteiterm_colnames].sum(axis=1)+opt_all.loc[:,STAI_positerm_colnames].sum(axis=1)\n",
    "#print(opt_all.loc[:,STAI_positerm_colnames])\n",
    "#print(x)\n",
    "BDI_colnames = ['BDI'+str(i+1) for i in range(21)]\n",
    "HPSV_colnames = ['HPSV'+str(i+1) for i in range(9)]\n",
    "RAHQ_colnames = ['RAHQ'+str(i+1) for i in range(18)]\n",
    "\n",
    "opt_all['BDI_all'] = opt_all.loc[:,BDI_colnames].sum(axis=1)\n",
    "opt_all['HPSV_all'] = opt_all.loc[:,HPSV_colnames].sum(axis=1)\n",
    "opt_all['RAHQ_all'] = opt_all.loc[:,RAHQ_colnames].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop some subjects.**\n",
    "\n",
    "<font face=\"Arial\" size=5 color=#DC143C> tips: </font>\n",
    "1. df.drop() to delete column or run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:37:45.344748Z",
     "start_time": "2020-01-15T12:37:45.337418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_all = opt_all.drop([53,34])# also remove sub34 because no avh basic infors.\n",
    "#rt, zero exist. sub42(30 zeros)，44(60 zeros)，52(90 and more zeros)\n",
    "#print(opt_all[opt_all['condition']=='pre']['acc'])\n",
    "#opt_all[np.abs(opt_all.acc-opt_all.acc.mean())<=(3*opt_all.acc.std())] \n",
    "#opt_all[~(np.abs(opt_all.acc-opt_all.acc.mean())>(3*opt_all.acc.std()))] \n",
    "#opt_all[opt_all.acc<0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=6 color=#0099ff>  5. check distribution and individual data points for gender, age and questionnaires </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:37:51.860363Z",
     "start_time": "2020-01-15T12:37:51.831983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = opt_all[['age','group']]\n",
    "iplot(df['age'].iplot(asFigure=True,kind='histogram', bins = 20,subplots=True, shape=(1, 1), filename='histogram-subplots'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check distribution and correlation among scale scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:32.579033Z",
     "start_time": "2020-01-15T11:37:32.566175Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_bygroup_bd = opt_all[['LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all','QPE_severity','TAI_all','BDI_all','group']].groupby(['group']).mean()\n",
    "# y_err_bd = opt_all[['LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all','QPE_severity','TAI_all','BDI_all','group']].groupby(['group']).sem()\n",
    "# df_bygroup_bd = df_bygroup_bd.T\n",
    "# y_err_bd = y_err_bd.T\n",
    "\n",
    "# df_bygroup_bd.plot(kind = 'bar',yerr = y_err_bd,width=0.8,style=['r','b'])#yerr = y_err_v,\n",
    "# plt.legend(loc='upper center', bbox_to_anchor=(1.2,1),ncol=1,fancybox=True,shadow=True)\n",
    "# #ylabel('bd')\n",
    "# show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:32.662821Z",
     "start_time": "2020-01-15T11:37:32.585192Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = opt_all[['LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all','QPE_severity','TAI_all','BDI_all']]\n",
    "iplot(df.iplot(asFigure=True,kind='histogram', bins = 20,subplots=True, shape=(7, 1), filename='histogram-subplots'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:32.763869Z",
     "start_time": "2020-01-15T11:37:32.670719Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all = df.drop_duplicates()\n",
    "print(df_all.columns)\n",
    "@interact(scale_name=df_all.columns)\n",
    "def iplot_scale(scale_name):\n",
    "    df = df_all[scale_name]\n",
    "    iplot(df.iplot(asFigure=True, kind='histogram', barmode='stack', bins=20, filename='basic-histogram'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:33.159672Z",
     "start_time": "2020-01-15T11:37:32.771149Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact(scale_name=df_all.columns)\n",
    "def iplot_scale(scale_name):\n",
    "    d = np.array(df_all[scale_name].dropna())\n",
    "    print(d)\n",
    "    sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, axes = plt.subplots(figsize=(7, 7), sharex=True)\n",
    "    sns.despine(left=True)\n",
    "    # Plot a historgram and kernel density estimate\n",
    "    sns.distplot(d, color=\"m\")\n",
    "    plt.setp(axes, yticks=[])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:33.588837Z",
     "start_time": "2020-01-15T11:37:33.163500Z"
    }
   },
   "outputs": [],
   "source": [
    "df = opt_all[['LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all','QPE_severity']]#,'TAI_all','BDI_all'\n",
    "iplot(df.scatter_matrix(asFigure=True,filename='scatter-matrix-subplot', bins=20, world_readable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:13:05.096958Z",
     "start_time": "2019-05-17T11:13:05.090028Z"
    }
   },
   "source": [
    "**Check distribution and correlation among scale scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:34.067236Z",
     "start_time": "2020-01-15T11:37:33.591397Z"
    }
   },
   "outputs": [],
   "source": [
    "df = opt_all[['LSHS_total','LSHS_AVH','HPSV_all','TAI_all','BDI_all']]#\n",
    "iplot(df.scatter_matrix(asFigure=True,filename='scatter-matrix-subplot', bins=20, world_readable=True))\n",
    "#help(df.scatter_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the distribution of ACC by condition and group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:34.151006Z",
     "start_time": "2020-01-15T11:37:34.072102Z"
    }
   },
   "outputs": [],
   "source": [
    "df_groupby = opt_all[['acc','group','condition']].groupby(['group','condition'])\n",
    "print(df_groupby.groups.keys())\n",
    "df_list = [df_groupby.get_group(key).reset_index()['acc'].rename() for key in df_groupby.groups.keys()]\n",
    "df = pd.concat(df_list,axis=1)\n",
    "#df = df.rename(columns={1:})\n",
    "iplot(df.iplot(asFigure=True,kind='histogram', barmode = 'stack',bins = 20, filename='histogram-subplots'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=6 color=#0099ff>  6. Calculate and plot variables in group level (gender,age,questionaires,rt,acc and other) </font>\n",
    "\n",
    "<font face=\"Arial\" size=5 color=#DC143C> tips: </font>\n",
    "1. use ** df.groupby() ** to devide data into sub-groups, then use formula (function) (mean, sem...) to manipulate data for each sub-group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:34.782112Z",
     "start_time": "2020-01-15T11:37:34.155616Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bygroup_basicinfor = opt_all[['gender','age','group']].groupby(['group']).mean()\n",
    "y_err_bd = opt_all[['gender','age','group']].groupby(['group']).sem()\n",
    "df_bygroup_basicinfor = df_bygroup_basicinfor.T\n",
    "y_err_bd = y_err_bd.T\n",
    "\n",
    "df_bygroup_basicinfor.plot(kind = 'bar',yerr = y_err_bd,width=0.8,style=['r','b'])#yerr = y_err_v,\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.2,1),ncol=1,fancybox=True,shadow=True)\n",
    "#ylabel('bd')\n",
    "show()\n",
    "\n",
    "#print(opt_all[['gender','group']])\n",
    "df_bygroup_basicinfor = opt_all[['age','gender','group']].groupby(['gender','group']).count()/2\n",
    "print(df_bygroup_basicinfor)\n",
    "#y_err_bd = opt_all[['gender','group']].groupby(['group']).sem()\n",
    "df_bygroup_basicinfor = df_bygroup_basicinfor.T\n",
    "#y_err_bd = y_err_bd.T\n",
    "\n",
    "df_bygroup_basicinfor.plot(kind = 'bar',width=0.8,style=['r','b'])#yerr = y_err_v,\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.2,1),ncol=1,fancybox=True,shadow=True)\n",
    "#ylabel('bd')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot scale scores between groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:34.868075Z",
     "start_time": "2020-01-15T11:37:34.786105Z"
    }
   },
   "outputs": [],
   "source": [
    "df = opt_all[['LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all','QPE_severity','TAI_all','BDI_all','group']]\n",
    "df = df.drop_duplicates()\n",
    "# color_list = ['rgba(93, 164, 214, 0.5)', 'rgba(93, 164, 214, 1)',  \n",
    "#               'rgba(44, 160, 101, 0.5)', 'rgba(44, 160, 101, 1)',  \n",
    "#               'rgba(255, 65, 54, 0.5)', 'rgba(255, 65, 54, 1)',  \n",
    "#               'rgba(207, 114, 255, 0.5)','rgba(207, 114, 255, 1)',  \n",
    "#               'rgba(127, 96, 0, 0.5)','rgba(127, 96, 0, 1)',  \n",
    "#               'rgba(255, 165, 0,0.5)','rgba(255, 165, 0,1)',  \n",
    "#               'rgba(255, 20, 147,0.5)','rgba(255, 20, 147,1)']\n",
    "color_list = ['rgba(93, 164, 214, 5)', 'rgba(93, 214, 214, 1)',  \n",
    "              'rgba(44, 160, 101, 5)', 'rgba(44, 210, 101, 1)',  \n",
    "              'rgba(255, 65, 54, 5)', 'rgba(255, 115, 54, 1)',  \n",
    "              'rgba(207, 114, 255, 5)','rgba(207, 164, 255, 1)',  \n",
    "              'rgba(127, 96, 0, 5)','rgba(127, 146, 0, 1)',  \n",
    "              'rgba(255, 165, 0,5)','rgba(255, 215, 0,1)',  \n",
    "              'rgba(255, 20, 147,5)','rgba(255, 70, 147,1)']\n",
    "scale_list = ['LSHS_total','LSHS_total','LSHS_AVH','LSHS_AVH','HPSV_all','HPSV_all','RAHQ_all','RAHQ_all',  \n",
    "              'QPE_severity','QPE_severity','TAI_all','TAI_all','BDI_all','BDI_all']\n",
    "group_list = ['High','Low']*7\n",
    "df_gls = pd.DataFrame({'gls':group_list,\n",
    "                       'sls':scale_list,\n",
    "                       'cls':color_list})\n",
    "data = [\n",
    "            {\n",
    "                'y': df[df['group']==group][clms],\n",
    "                'type': 'box', \n",
    "                'boxpoints':'all',\n",
    "                #'fillcolor':\n",
    "                'marker': dict(color = df_gls[(df_gls['gls']==group)&(df_gls['sls']==clms)]['cls'].values[0]),\n",
    "                'boxmean':'sd',\n",
    "                'name': clms + '_' + group\n",
    "            } \n",
    "                for clms in ['LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all','QPE_severity','TAI_all','BDI_all']\n",
    "            for group in ['High','Low']\n",
    "        ]\n",
    "iplot(Figure(data=Data(data),layout = Layout(showlegend=False)))\n",
    "#py.image.save_as(Figure(data=Data(data)), filename='a-simple-plot.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate difference of scale scores between groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:34.979258Z",
     "start_time": "2020-01-15T11:37:34.871237Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a list of all columns in the dataframe without the Group column\n",
    "df = opt_all[['LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all','QPE_severity','TAI_all','BDI_all','group']]\n",
    "df = df.drop_duplicates()\n",
    "column_list = [x for x in df.columns if x != 'group']\n",
    "# create an empty dictionary\n",
    "t_test_results = {}\n",
    "# loop over column_list and execute code explained above\n",
    "for column in column_list:\n",
    "    group1 = df.where(df.group== 'High')[column].dropna()\n",
    "    group2 = df.where(df.group== 'Low')[column].dropna()\n",
    "    print group1.count(),group2.count()\n",
    "    # add the output to the dictionary \n",
    "    t_test_results[column] = ttest_ind(group1,group2)\n",
    "results_df = pd.DataFrame.from_dict(t_test_results,orient='Index')\n",
    "results_df.columns = ['statistic','pvalue']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=6 color=#0099ff>  7.Plot bar graph and do anova and t-test statistics for RT and ACC. </font>\n",
    "\n",
    "**Plot acc by conditon and group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:35.279349Z",
     "start_time": "2020-01-15T11:37:34.982542Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove subjects with very high fa, let's say more than 40\n",
    "opt_all = opt_all[~opt_all.index.isin(opt_all[opt_all['fa']>40].index)]\n",
    "df_bygroup_acc = opt_all[['acc','group','condition']].groupby(['group','condition']).mean()\n",
    "y_err_acc = opt_all[['acc','group','condition']].groupby(['group','condition']).sem()\n",
    "df_bygroup_acc = df_bygroup_acc.T\n",
    "y_err_acc = y_err_acc.T\n",
    "\n",
    "df_bygroup_acc.plot(kind = 'bar',yerr = y_err_acc,width=0.8,style=['r--','r','b--','b'])#yerr = y_err_v,\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.2,1),ncol=1,fancybox=True,shadow=True)\n",
    "#ylabel('acc')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:35.341141Z",
     "start_time": "2020-01-15T11:37:35.284383Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print opt_all\n",
    "opt_all_tmp = opt_all.copy()\n",
    "opt_all_tmp.reset_index(inplace=True)\n",
    "sublist_all = opt_all_tmp[['Subject','group']].drop_duplicates()\n",
    "sublist_paired = pd.read_table('Test_Preproc_GHY_SE.txt',header=None)\n",
    "#?pd.read_table\n",
    "sublist_paired.columns=['DNA_No','DNA_format_No']\n",
    "sublist_paired['DNA_No'] = sublist_paired['DNA_No'].apply(lambda x: int(x[4:]))\n",
    "sublist_paired = sublist_paired.rename(columns={'DNA_No':'Subject'})\n",
    "sublist_FA40_all = sublist_all.set_index('Subject').join(sublist_paired.set_index('Subject'))\n",
    "sublist_FA40_all[sublist_FA40_all.group=='High']['DNA_format_No'].to_csv('sublist_high_all.txt',index=False)\n",
    "sublist_FA40_all[sublist_FA40_all.group=='Low']['DNA_format_No'].to_csv('sublist_low_all.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**ANOVA for ACC**\n",
    "\n",
    "\n",
    "<font face=\"Arial\" size=5 color=#DC143C> tips: </font>\n",
    "1. organize the data in the format requred by R fomula\n",
    "2. use rpy2 interface to use R\n",
    "3. use pivot to transform long format into wide format data, then save csv data to check statistic by using JASP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:35.516800Z",
     "start_time": "2020-01-15T11:37:35.345284Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "df_se_pra = opt_all.copy()\n",
    "df_se_pra.reset_index(inplace=True)\n",
    "for bd_val in ['acc']:\n",
    "    print('This is anova for %s:') %(bd_val)\n",
    "    df_bd_val = df_se_pra[['Subject','group','condition',bd_val]]#\n",
    "\n",
    "    df_bd_val = df_bd_val.drop_duplicates()\n",
    "    # remove all subjects including na\n",
    "    sublist_na = df_bd_val['Subject'][df_bd_val.iloc[:,-1].isna()].values\n",
    "    df_bd_val = df_bd_val[~df_bd_val['Subject'].isin(sublist_na)]\n",
    "    #print df_bd_val\n",
    "#     sublist_bls = df[df.duplicated(keep='first')]['Subject'].values\n",
    "#     df_bd_val = df_bd_val[df_bd_val['Subject'].isin(sublist_bls)]    \n",
    "    #df2=df_bd_val.pivot(index='Subject', columns='condition', values='rt')\n",
    "    \n",
    "    r_df_bd_val = pandas2ri.py2ri(df_bd_val) \n",
    "    #print(r_df_bd_val)\n",
    "    if bd_val == 'acc':\n",
    "            %R -i r_df_bd_val r_df_bd_val$Subject <-factor(r_df_bd_val$Subject);\\\n",
    "            aov_group_conditon <- aov(acc ~ group*condition + Error(Subject/condition), data=r_df_bd_val);\\\n",
    "            print(summary(aov_group_conditon));print(model.tables(aov_group_conditon, \"means\"))\n",
    "   \n",
    "    # tranform data from long format into wide format. \n",
    "    df2=df_bd_val.pivot(index='Subject', columns='condition', values='acc')\n",
    "    df2 = df2.join(df_bd_val[df_bd_val.condition=='pre'][['Subject','group']].set_index('Subject'))\n",
    "    #print df2\n",
    "    df2.to_csv('df_acc4anova.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-test for ACC to check interaction pattern (simple effect)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:35.588411Z",
     "start_time": "2020-01-15T11:37:35.521174Z"
    }
   },
   "outputs": [],
   "source": [
    "for con in ['pre','unp']:\n",
    "    df = opt_all[['acc','group','condition']]\n",
    "    df = df[df['condition']==con][['acc','group']]\n",
    "    #print(df)\n",
    "    column_list = [x for x in df.columns if x != 'group']\n",
    "    # create an empty dictionary\n",
    "    t_test_results = {}\n",
    "    # loop over column_list and execute code explained above\n",
    "    for column in column_list:\n",
    "        group1 = df.where(df.group== 'High').dropna()[column]\n",
    "        group2 = df.where(df.group== 'Low').dropna()[column]\n",
    "        print group1.count(),group2.count()\n",
    "        # add the output to the dictionary \n",
    "        #print(group1,group2)\n",
    "        t_test_results[column] = ttest_ind(group1,group2)\n",
    "    results_df = pd.DataFrame.from_dict(t_test_results,orient='Index')\n",
    "    results_df.columns = ['statistic','pvalue']\n",
    "    print con,'\\n',results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:35.909588Z",
     "start_time": "2020-01-15T11:37:35.591892Z"
    }
   },
   "outputs": [],
   "source": [
    "df_bygroup_rt = opt_all[['rt','group','condition']].groupby(['group','condition']).mean()\n",
    "y_err_rt = opt_all[['rt','group','condition']].groupby(['group','condition']).sem()\n",
    "df_bygroup_rt = df_bygroup_rt.T\n",
    "y_err_rt = y_err_rt.T\n",
    "\n",
    "df_bygroup_rt.plot(kind = 'bar',yerr = y_err_rt,width=0.8,style=['r--','r','b--','b'])#yerr = y_err_v,\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.2,1),ncol=1,fancybox=True,shadow=True)\n",
    "#ylabel('acc')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:35.983523Z",
     "start_time": "2020-01-15T11:37:35.916003Z"
    }
   },
   "outputs": [],
   "source": [
    "df_se_pra = opt_all.copy()\n",
    "df_se_pra.reset_index(inplace=True)\n",
    "for bd_val in ['rt']:\n",
    "    print('This is anova for %s:') %(bd_val)\n",
    "    df_bd_val = df_se_pra[['Subject','group','condition',bd_val]]#\n",
    "    df_bd_val = df_bd_val.drop_duplicates()\n",
    "    # remove all subjects including na\n",
    "    sublist_na = df_bd_val['Subject'][df_bd_val.iloc[:,-1].isna()].values\n",
    "    df_bd_val = df_bd_val[~df_bd_val['Subject'].isin(sublist_na)]\n",
    "#     sublist_bls = df[df.duplicated(keep='first')]['Subject'].values\n",
    "#     df_bd_val = df_bd_val[df_bd_val['Subject'].isin(sublist_bls)]    \n",
    "    #df2=df_bd_val.pivot(index='Subject', columns='condition', values='rt')\n",
    "    df_bd_val.to_csv('tmp.csv')\n",
    "    r_df_bd_val = pandas2ri.py2ri(df_bd_val) \n",
    "     #print(r_df_bd_val)\n",
    "    if bd_val == 'rt':\n",
    "        %R -i r_df_bd_val r_df_bd_val$Subject <-factor(r_df_bd_val$Subject);aov_group_conditon <- aov(rt ~ group*condition + Error(Subject/condition), data=r_df_bd_val);print(summary(aov_group_conditon));print(model.tables(aov_group_conditon, \"means\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:36.066548Z",
     "start_time": "2020-01-15T11:37:35.987899Z"
    }
   },
   "outputs": [],
   "source": [
    "for con in ['pre','unp']:\n",
    "    df = opt_all[['rt','group','condition']]\n",
    "    df = df[df['condition']==con][['rt','group']]\n",
    "    #print(df)\n",
    "    column_list = [x for x in df.columns if x != 'group']\n",
    "    # create an empty dictionary\n",
    "    t_test_results = {}\n",
    "    # loop over column_list and execute code explained above\n",
    "    for column in column_list:\n",
    "        group1 = df.where(df.group== 'High').dropna()[column]\n",
    "        group2 = df.where(df.group== 'Low').dropna()[column]\n",
    "        print group1.count(),group2.count()\n",
    "        # add the output to the dictionary \n",
    "        #print(group1,group2)\n",
    "        t_test_results[column] = ttest_ind(group1,group2)\n",
    "    results_df = pd.DataFrame.from_dict(t_test_results,orient='Index')\n",
    "    results_df.columns = ['statistic','pvalue']\n",
    "    print con, '\\n',results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:36.161494Z",
     "start_time": "2020-01-15T11:37:36.070563Z"
    }
   },
   "outputs": [],
   "source": [
    "for bd_val  in ['acc','rt']:\n",
    "    df = opt_all[[bd_val,'group','condition']]\n",
    "    df = df.drop_duplicates()\n",
    "    color_list = ['rgba(255, 65, 54, 5)', 'rgba(255, 115, 54, 1)',  \n",
    "                  'rgba(44, 160, 101, 5)', 'rgba(44, 210, 101, 1)']\n",
    "    scale_list = ['unp','pre','unp','pre']\n",
    "    group_list = ['High']*2+['Low']*2\n",
    "    df_gls = pd.DataFrame({'gls':group_list,\n",
    "                           'sls':scale_list,\n",
    "                           'cls':color_list})\n",
    "    #print(df_gls)\n",
    "#     for group in ['High','Low']:\n",
    "#         for clms in ['unp','pre']:\n",
    "#             print(group,clms)\n",
    "            #print(df_gls[(df_gls['gls']==group)&(df_gls['sls']==clms)]['cls'].values[0])\n",
    "\n",
    "    data = [\n",
    "                {\n",
    "                    'y': df[(df['group']==group)&(df['condition']==clms)][bd_val],\n",
    "                    'type': 'box', \n",
    "                    'boxpoints':'all',\n",
    "                    #'fillcolor':\n",
    "                    'marker': dict(color = df_gls[(df_gls['gls']==group)&(df_gls['sls']==clms)]['cls'].values[0]),\n",
    "                    'boxmean':'sd',\n",
    "                    'name': clms + '_' + group\n",
    "                }   \n",
    "                    for group in ['High','Low']\n",
    "                for clms in ['unp','pre']\n",
    "            ]\n",
    "    iplot(Figure(data=Data(data),layout = Layout(width=500,height=500,title=bd_val,showlegend=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font face=\"Arial\" size=5 color=#DC143C> use dPrime function to calulate singal detection theroy measures, d,beta,c,Ad. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:36.192135Z",
     "start_time": "2020-01-15T11:37:36.164787Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division # Not neccessary in Python 3 and later\n",
    "from scipy.stats import norm\n",
    "from math import exp,sqrt\n",
    "Z = norm.ppf\n",
    " \n",
    "def dPrime(hits, misses, fas, crs):\n",
    "    # Floors an ceilings are replaced by half hits and half FA's\n",
    "    halfHit = 0.5/(hits+misses)\n",
    "    halfFa = 0.5/(fas+crs)\n",
    " \n",
    "    # Calculate hitrate and avoid d' infinity\n",
    "    hitRate = hits/(hits+misses)\n",
    "    if hitRate == 1: hitRate = 1-halfHit\n",
    "    if hitRate == 0: hitRate = halfHit\n",
    " \n",
    "    # Calculate false alarm rate and avoid d' infinity\n",
    "    faRate = fas/(fas+crs)\n",
    "    if faRate == 1: faRate = 1-halfFa\n",
    "    if faRate == 0: faRate = halfFa\n",
    " \n",
    "    # Return d', beta, c and Ad'\n",
    "    out = {}\n",
    "    out['d'] = Z(hitRate) - Z(faRate)\n",
    "    out['beta'] = exp((Z(faRate)**2 - Z(hitRate)**2)/2)\n",
    "    out['c'] = -(Z(hitRate) + Z(faRate))/2\n",
    "    out['Ad'] = norm.cdf(out['d']/sqrt(2))\n",
    "    return out\n",
    "dPrime(0.8,0.2,0.7,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:36.451725Z",
     "start_time": "2020-01-15T11:37:36.196127Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#opt_all = opt_all.set_index('Subject')\n",
    "opt_all['fa'] = opt_all['fa'].apply(lambda x:x/60)\n",
    "acc_2cond = opt_all[['group','condition','fa','acc']].groupby(['Subject'])['acc'].mean()\n",
    "opt_all.reset_index(inplace=True) \n",
    "opt_all =opt_all.set_index(['Subject'])\n",
    "opt_all = opt_all.join(acc_2cond,rsuffix='_2cond')\n",
    "opt_all.reset_index(inplace=True) \n",
    "opt_all =opt_all.set_index(['Subject'])\n",
    "opt_all.reset_index(inplace=True) \n",
    "df_dPrime = [dPrime(opt_all['acc_2cond'][idx],1-opt_all['acc_2cond'][idx],opt_all['fa'][idx],1-opt_all['acc_2cond'][idx]) for idx in opt_all.index]\n",
    "opt_all = opt_all.join(pd.DataFrame(df_dPrime))\n",
    "opt_all = opt_all.set_index('Subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:36.460358Z",
     "start_time": "2020-01-15T11:37:36.454295Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#opt_all[['group','condition','d','c','Ad','beta','fa']].to_csv('tmp.csv')\n",
    "opt_all.reset_index(inplace=True)\n",
    "#opt_all = opt_all[opt_all['fa']<=40/60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:36.512341Z",
     "start_time": "2020-01-15T11:37:36.464717Z"
    }
   },
   "outputs": [],
   "source": [
    "# themes =  cf.getThemes()\n",
    "# print(themes)\n",
    "# cf.set_config_file(theme=themes[6])\n",
    "#print(opt_all)\n",
    "bd_str = 'fa'\n",
    "#opt_all.to_csv('tmp1.csv')\n",
    "sublist_na = opt_all['Subject'][opt_all.loc[:,bd_str].isna()].values\n",
    "df_se_pra_nona = opt_all[~opt_all['Subject'].isin(sublist_na)]\n",
    "df_tmp = df_se_pra_nona[(df_se_pra_nona['condition']=='unp')][['Subject','group','d','c','Ad','beta','fa']]\n",
    "df_tmp = df_tmp.drop_duplicates()\n",
    "df_tmp.to_csv('tmp.csv')\n",
    "data = [\n",
    "            {\n",
    "                'y': df_tmp[(df_tmp['group']==group)][bd_str],\n",
    "                'type': 'box', \n",
    "                'boxpoints':'all',\n",
    "                'name': bd_str + '_' + str(group)\n",
    "            } \n",
    "                for bd_str in  ['d','c','Ad','beta']\n",
    "            for group in ['High','Low']\n",
    "        ]\n",
    "iplot(Figure(data=Data(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**correlation between scales score and behaviorial measurements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:39.928942Z",
     "start_time": "2020-01-15T11:37:36.515648Z"
    }
   },
   "outputs": [],
   "source": [
    "#%debug\n",
    "#bd_str = 'LSHS_AVH'\n",
    "\n",
    "#opt_all.to_csv('tmp1.csv')\n",
    "sublist_na = opt_all['Subject'][(opt_all.loc[:,'BDI_all'].isna())|(opt_all.loc[:,'LSHS_AVH'].isna())].values\n",
    "df_se_pra_nona = opt_all[~opt_all['Subject'].isin(sublist_na)]\n",
    "df_tmp = df_se_pra_nona[(df_se_pra_nona['condition']=='unp')][['Subject','LSHS_total','LSHS_AVH',  \n",
    "                                                               'HPSV_all','RAHQ_all','QPE_severity',  \n",
    "                                                               'TAI_all','BDI_all','d',  \n",
    "                                                               'Ad','c','beta','fa']]  \n",
    "                                                               \n",
    "df = df_tmp.drop_duplicates()\n",
    "df.to_csv('tmp.csv')\n",
    "from scipy.stats import pearsonr\n",
    "#print(pearsonr(df['LSHS_total'],df['d']))\n",
    "r_p = [pearsonr(df[cls1],df[cls2]) for cls1 in df.columns for cls2 in df.columns]\n",
    "r, p = zip(*r_p)\n",
    "bd_names = ['Subject','LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all',  \n",
    "            'QPE_severity','TAI_all','BDI_all','d','Ad','c','beta','fa']\n",
    "r = pd.DataFrame(np.array(r).reshape((13,13)),columns=bd_names,index = bd_names)\n",
    "p = pd.DataFrame(np.array(p).reshape((13,13)),columns=bd_names,index = bd_names)\n",
    "print(p[['LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all','QPE_severity']])\n",
    "#print(p)\n",
    "\n",
    "\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "# Set up the matplotlib figure\n",
    "f, axes = plt.subplots(2, 4, figsize=(15, 15))\n",
    "sns.despine(left=True)\n",
    "cnt1 = 0\n",
    "for clms1 in ['LSHS_AVH','RAHQ_all']:\n",
    "    cnt2 = 0\n",
    "    for clms2 in ['d','Ad','c','beta']:#,'fa'\n",
    "        b= sns.regplot(x=clms1, y=clms2, data=df,ci=68,ax=axes[cnt1, cnt2])\n",
    "        #sns.set_xlabel(\"X Label\",fontsize=30)\n",
    "        b.set_xlabel(clms2,fontsize=18*2)\n",
    "        b.set_ylabel(clms1,fontsize=18*2)\n",
    "        cnt2 +=1\n",
    "    cnt1 += 1 \n",
    "#plt.setp(axes, yticks=[])\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_irt_avh.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:40.059273Z",
     "start_time": "2020-01-15T11:37:39.931877Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get a list of all columns in the dataframe without the Group column\n",
    "df = opt_all[(opt_all['condition']=='unp')][['group','d','c','Ad','beta','fa']]\n",
    "df.to_csv('tmp.csv')\n",
    "column_list = [x for x in df.columns if x != 'group']\n",
    "# create an empty dictionary\n",
    "t_test_results = {}\n",
    "# loop over column_list and execute code explained above\n",
    "for column in column_list:\n",
    "    group1 = df.where(df.group== 'High').dropna()[column]\n",
    "    group2 = df.where(df.group== 'Low').dropna()[column]\n",
    "    # add the output to the dictionary \n",
    "    t_test_results[column] = ttest_ind(group1,group2)\n",
    "results_df = pd.DataFrame.from_dict(t_test_results,orient='Index')\n",
    "results_df.columns = ['statistic','pvalue']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:40.143262Z",
     "start_time": "2020-01-15T11:37:40.077832Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_all_tmp = opt_all[(opt_all['condition']=='unp')][['Subject','group','d','c','Ad','beta','fa',\n",
    "                                        'LSHS_total','LSHS_AVH','HPSV_all','RAHQ_all',\n",
    "                                         'QPE_severity','TAI_all','BDI_all',]]\n",
    "# %store opt_all_tmp\n",
    "# del opt_all_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Arial\" size=6 color=#0099ff>  8.Advanced machine learning approach. </font>\n",
    "\n",
    "## To demostrate how easy to implement a machine learning model in python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:40.169750Z",
     "start_time": "2020-01-15T11:37:40.157612Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_all_tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:37:40.206363Z",
     "start_time": "2020-01-15T11:37:40.175018Z"
    }
   },
   "outputs": [],
   "source": [
    "print opt_all_tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:02.135695Z",
     "start_time": "2020-01-15T11:40:02.124610Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_all_tmp['HPSV_all'].fillna(X_train['HPSV_all'].mean(), inplace=True)\n",
    "opt_all_tmp['RAHQ_all'].fillna(X_train['RAHQ_all'].mean(), inplace=True)\n",
    "opt_all_tmp['QPE_severity'].fillna(X_train['QPE_severity'].mean(), inplace=True)\n",
    "opt_all_tmp['TAI_all'].fillna(X_train['TAI_all'].mean(), inplace=True)\n",
    "opt_all_tmp['BDI_all'].fillna(X_train['BDI_all'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:03.299151Z",
     "start_time": "2020-01-15T11:40:03.288673Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# selected_features = [u'd',u'c',u'Ad',u'beta',u'fa',u'LSHS_total',u'LSHS_AVH',\\\n",
    "#                      u'HPSV_all',u'RAHQ_all', u'QPE_severity',u'TAI_all',u'BDI_all']\n",
    "selected_features = [u'd',u'c',u'Ad',u'beta',u'fa',u'LSHS_total',u'HPSV_all',u'RAHQ_all', u'QPE_severity',u'TAI_all',u'BDI_all']\n",
    "X = opt_all_tmp[selected_features]\n",
    "y = opt_all_tmp['group']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:04.196105Z",
     "start_time": "2020-01-15T11:40:04.182819Z"
    }
   },
   "outputs": [],
   "source": [
    "print opt_all_tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:05.151564Z",
     "start_time": "2020-01-15T11:40:04.890717Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(rfc, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:06.008020Z",
     "start_time": "2020-01-15T11:40:05.909554Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbc = XGBClassifier()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "cross_val_score(xgbc, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:06.604677Z",
     "start_time": "2020-01-15T11:40:06.539730Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc.fit(X_train,y_train)\n",
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:07.182767Z",
     "start_time": "2020-01-15T11:40:07.175500Z"
    }
   },
   "outputs": [],
   "source": [
    "range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:07.858634Z",
     "start_time": "2020-01-15T11:40:07.851794Z"
    }
   },
   "outputs": [],
   "source": [
    "len([range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:08.439299Z",
     "start_time": "2020-01-15T11:40:08.433214Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array([0.])\n",
    "#np.concatenate((np.array([0.]), cdf, np.array([1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:09.036834Z",
     "start_time": "2020-01-15T11:40:09.028079Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.randn(len(range(10)))*(2**-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T11:40:09.644720Z",
     "start_time": "2020-01-15T11:40:09.627353Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgbc.fit(X_train,y_train)\n",
    "xgbc.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
